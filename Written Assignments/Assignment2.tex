\documentclass{article}

\usepackage{booktabs} 
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{amsmath}

\title {Assignment 2}
\date{}
\author{Wendy Nieuwkamer}

\begin{document}

\maketitle

\section{Question 1}
This question is about \textit{vectorization},i.e. writing expressions
in matrix-vector form. The goal is to vectorize the update rule
for multivariate linear regression.\\

Let $\theta$ be the parameter vector $\theta = \begin{pmatrix} \theta_0 & \theta_1 & \cdots & \theta_n \\ \end{pmatrix} ^T$ and let the i-th data vector be: $ x^{(i)} = \begin{pmatrix} x_0 & x_1 & \cdots & x_n \\\end{pmatrix}^T$ where $ x_0 = 1$.

\subsection{Write the hypothesis function $h_\theta(x)$ as a vectorial expression.}

The summation notation for the hypothesis function is:

\begin{equation*}
	h_\theta (x^{(i)}) = \theta_0 x_0^{(i)} + \theta_1 x_1^{(i)}+ \theta_2 x_2^{(i)}+ ... + \theta_n x_n^{(i)}
\end{equation*}\\

This is the same as the result of the following matrix multiplication:
\begin{equation}
	h_\theta (x^{(i)}) = \theta^T x^{(i)},
\end{equation}

which is a vectorial expression.

\subsection{What is the vectorized expression for the cost function: $J(\theta)$?}
The cost function in the notation used up to now:

\begin{equation*}
		J(\theta) = \frac{1}{2m} \sum\limits_{i=1}^m (h _{\theta} (x ^{(i)}) - y ^{(i)})^2.
\end{equation*}
If we simply insert the vectorial notation of the hypothesis function from last question (1) we get:

\begin{equation*}
		J(\theta) = \frac{1}{2m} \sum\limits_{i=1}^m (\theta^T x^{(i)} - y ^{(i)})^2.
\end{equation*}

\subsection{What is the vectorized expression for the gradient of the cost function?}
\textit{i.e. what is:}

\begin{equation*}
\frac{\delta J(\theta)}{\delta \theta} = \begin{pmatrix} \frac{\delta J(\theta)}{\delta \theta_0} \\ \vdots \\ \frac{\delta J(\theta)}{\delta \theta_n} \end{pmatrix}
\end{equation*}
\textit{Again the explicit summation over the data vectors from the
learning set is allowed here.}\\

The notation we used up until now is:

\begin{equation*}
\frac{\delta J(\theta)}{\delta \theta_j}= \frac{1}{m} \sum\limits_{i=1}^m (h _{\theta} (x ^{(i)}) - y ^{(i)})x_j^{(i)}.
\end{equation*}

If we integrate the vectorized notation of the hypothesis (1) again we get:

\begin{equation*}
\frac{\delta J(\theta)}{\delta \theta_j}= \frac{1}{m} \sum\limits_{i=1}^m ( \theta^T x^{(i)}  - y ^{(i)})x_j^{(i)}.
\end{equation*}

Substituting this summation for the $\delta$ notation will give us the following vector:

\begin{equation}
\frac{\delta J(\theta)}{\delta \theta} = \frac{1}{m}\begin{pmatrix}	\sum\limits_{i=1}^m ( \theta^T x^{(i)}  - y ^{(i)})\\ 
									     		\sum\limits_{i=1}^m ( \theta^T x^{(i)}  - y ^{(i)})x_1^{(i)} \\ 
										      	 \vdots \\
										 	\sum\limits_{i=1}^m ( \theta^T x^{(i)}  - y ^{(i)})x_n^{(i)}
						\end{pmatrix}
\end{equation}

\subsection{What is the vectorized expression for the $\theta$ update rule in the gradient descent procedure?}
The original notation for the update rule for one theta was:

\begin{equation*}
\theta_j = \theta_j - \alpha \frac{1}{m}  \sum\limits_{i=1}^m (h _{\theta} (x ^{(i)}) - y ^{(i)})x_i^{(i)}
\end{equation*}

By writing it in vector notation we update the entire theta instead of each element separately. Using the formulas from (1) and (2) we get:

\begin{equation*}
\theta = \theta - \alpha \frac{1}{m}\begin{pmatrix}	\sum\limits_{i=1}^m ( \theta^T x^{(i)}  - y ^{(i)})\\ 
									     		\sum\limits_{i=1}^m ( \theta^T x^{(i)}  - y ^{(i)})x_1^{(i)} \\ 
										      	 \vdots \\
										 	\sum\limits_{i=1}^m ( \theta^T x^{(i)}  - y ^{(i)})x_n^{(i)}
						\end{pmatrix}
\end{equation*}




\pagebreak

\section{Question 2}
\textit{Consider events with two binary outcomes, X and Y. We encode the two values as 0 and 1. We can represent
the outcomes of an experiment in a 2 by 2 frequency table:}

\begin{table}[h!]
\centering
\begin{tabular}{c|c c}
~ & X=0 & X=1 \\
\hline
Y=0 & a & b \\
Y=1 & c & d \\
\end{tabular}
\end{table}

\textit{Express the following as a function of a/b/c/d:}

\subsection{$P(X=0)$}
$X=0$ for both the combinations $ X=0, Y=0$, and $X=0, Y=1$. Thus, we are looking at $a$ and $c$. Then, according to the standard formula:

\begin{equation*}
P(X=0) = \frac{a + c}{a+b+c+d}
\end{equation*}

\subsection{$P(X=1|Y=0)$}



\end{document}