\documentclass[12pt, a4paper]{article}

\usepackage{booktabs} 
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{amsmath}

\title {Assignment 1}
\date{16-9-2016}
\author{Wendy Nieuwkamer}

\begin{document}

\maketitle

\section{Question 1}
Suppose that we have historical data of result of soccer matches of teams playing against Ajax. We wantto
use this information to learn to predict at a certain moment whether a team will win, lose or draw against
Ajax. Our approach will be based on Machine Learning.

\subsection{Define the given and the goal of the prediction task.
Classify the learning task.}

Given are the results of soccer matches, the goal is to decide whether a team will win, lose or draw when playing. 
The learning task is supervised, as we are given inputs with their wanted result. It is a classification problem as 
every input should be classified as one of three possibilities.


\subsection{What would be the form of training data for the learning task? Give a small training set.}
An example of a training set could be the following table:

\begin{table}
\centering
\begin{tabular}{c|c}
   team & results \\
   \hline
   1 & win\\
   2 & draw\\
   3 & lose\\
   1 & lose\\ 
   3 & draw\\
   4 & win\\
\end{tabular}
\caption{training examples}
\end{table}

\section{Question 2}
Use the training data in the table to answer all the subquestions. 

\begin{table}[h!]
\centering
\begin{tabular}{c|c}
   x & y \\
   \hline
   3 & 6\\
   5 & 7\\
   6 & 10
\end{tabular}
\end{table}	

	\subsection{Manually  calculate two iterations of the gradient descent algorithm
	for univariate linear regression function. }
	
	\textit{Initialize the parameters such that the regression function
	passes through the origin (0, 0) and has an angle of 45 degrees. Use a learning rate of 0.1. Give the
	intermediate results of your calculations and also compute the mean-squared error of the function
	after 2 iterations.}\\
	
	\textbf{Gradient Descent Algorithm}\\
	
	Repeat until convergence \{
	\begin{align*}
	\theta _0 &:= \theta _0 - \alpha \frac{1}{m} \sum\limits_{i=1}^m (h _{\theta} (x ^{(i)}) - y ^{(i)})\\
	\theta _1 &:= \theta _1 - \alpha \frac{1}{m} \sum\limits_{i=1}^m (h _{\theta} (x ^{(i)}) - y ^{(i)})x^{(i)} \\
	\end{align*}
	\}\\
	
	The hypothesis $h_\theta$ is:
	\begin{equation*}
	h_\theta (x^{(i)}) = \theta_0 = \theta_1 x^{(i)}
	\end{equation*}\\
	
	For this exercise the learning rate $\alpha=0.1$ and the amount of learning examples $m=3$ are given. The line that passes through the origin and has an angle of 45 degrees relative to 		the x-axis is $f(x) = x$, thus our starting values are $\theta_0 = 0$ and $\theta_1 = 1$.
	
	\paragraph{First Iteration}
	First, calculate the new values for $\theta_0$ and $\theta_1$.
	
		\begin{align*}
		\theta_0 &= 0 - 0.1 \cdot \frac{1}{3}\sum\limits_{i=1}^3 (x ^{(i)} - y ^{(i)})\\
		&= -\frac{1}{30} (-3 + -2 + -4)\\
		&= 0.3 \\
		\\
		\theta_1&=1-0.1 \cdot \frac{1}{3}\sum\limits_{i=1}^3 (x ^{(i)} - y ^{(i)})x^{(i)}\\
		&=1-\frac{1}{30}(-9+-10+-24)\\
		&=1+\frac{43}{30}=\frac{73}{30}=2.34
		\end{align*}
		
		Then, update $\theta_0$ and $\theta_1$.
		
		\begin{align*}
		\theta_0 &:= 0.3 \\
		\theta_1&:=2.34
		\end{align*}
	
	\paragraph{Second Iteration}
	The values for $m$ and $\alpha$ are still the same, but now we use the new values for $\theta_0$ and $\theta_1$. Again, first the calculations.
	
		\begin{align*}
		\theta_0 &= 0.30 - 0.1 \cdot \frac{1}{3}\sum\limits_{i=1}^3 (0.30+2.34x ^{(i)} - y ^{(i)})\\
		&= 0.30-\frac{1}{30} (1.6+5.47+4.9)\\
		&= 0.30 - 0.40 = -0.10\\
		\\
		\theta_1&=2.34-0.1 \cdot \frac{1}{3}\sum\limits_{i=1}^3 (0.30+2.34x ^{(i)} - y ^{(i)})x^{(i)}\\
		&=2.34-\frac{1}{30}(8.15+27.33+29.40)\\
		&=2.34-2.16=0.18
		\end{align*}
		
		\pagebreak
		Then, update $\theta_0$ and $\theta_1$.
		
		\begin{align*}
		\theta_0 &:= -0.10\\
		\theta_1&:=0.18
		\end{align*}
	
	\paragraph{Mean-Squared Error}
	Now we can calculate the mean-squared error to see how good our hypothesis $h_\theta(x^{(i)}) = -0.10 + 0.18x^{(i)}$ is. The formula we use is:
	
		\begin{equation*}
		J(\theta_0, \theta_1) = \frac{1}{2m} \sum\limits_{i=1}^m (h _{\theta} (x ^{(i)}) - y ^{(i)})^2
		\end{equation*}
		
		This function is also called the cost function. We can now insert our values $m=3$, $\theta_0=-0.10$ and $\theta_1=0.18$.
		
		\begin{align*}
		J(\theta_0, \theta_1) &= \frac{1}{6} \sum\limits_{i=1}^3 (0.18x ^{(i)} - 0.10 - y ^{(i)})^2\\
		 &= \frac{1}{6} (30.91+38.44+81.36)\\
		&=25.12
		\end{align*}
		
		Thus, the mean-squared error for $m=3$, $\theta_0=-0.10$, $\theta_1=0.18$ and the given training examples is 25.12.
	
	\subsection{Convert the data to z-scores, repeat the calculations above and compare the results.}
	To calculate the z-scores use mean $\mu=0$ and standard deviation $\sigma=1$. The formula for calculating a z-score is:
	
	\begin{equation*}
	Z = \frac{X-\mu}{\sigma}
	\end{equation*}
	
	Where $X$ is the input value of the training example. Inserting our $\mu$ and $\sigma$ into this formula gives:
	
	\begin{equation*}
	Z = \frac{X-0}{1} = X
	\end{equation*}
	
	Redoing the calculations would not be very useful as with this $\mu$ and $\sigma$ the Z-score would be the same as the original input. It would be more useful to choose $\mu=4.5$ 
	and $\sigma=1.125$ for $x$ as this actually changes the values.

	\pagebreak

\section{Question 3}

\section{Question 4}
\textit{Derive an equation that can be used to find the optimal value of the parameter $\theta_1$ for
univariate linear regression without doing gradient descent. This can be done by setting the value of the
derivative equal to 0. You may assume that the value of $\theta_0$ is fixed.}

To find a formula for $\theta_1$ we take zero as the value of the derivative relative to $\theta_1$ of the cost function: 

\begin{equation*}
\frac{\delta}{\delta\theta_1}J(\theta_0, \theta_1) = 0
\end{equation*}

We have seen this derivative before as a part of the gradient descent algorithm, so the next step is easy. 

\begin{equation*}
 \frac{1}{m} \sum\limits_{i=1}^m (h _{\theta} (x ^{(i)}) - y ^{(i)})x^{(i)} = 0
\end{equation*}

Now, in order to simplify the equation, we first multiply both sides by $m$ (1), write out the hypothesis $h_\theta$ (2) and write out the multiplication with $x^{(i)}$ (3).

\begin{align}
&\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x^{(i)} &= 0\\
&\sum\limits_{i=1}^m(\theta_0+\theta_1x^{(i)} - y^{(i)})x^{(i)} &= 0\\
&\sum\limits_{i=1}^m(\theta_0x^{(i)} + \theta_1(x^{(i)})^2 - y^{(i)}x^{(i)}) &= 0
\end{align}

Now, we can breakup the summation (4), factor out $\theta_1$ (5) and the summation without $\theta_1$ to the right side of the equation (6).

\begin{align}
\sum\limits_{i=1}^m(\theta_1(x^{(i)})^2 ) + \sum\limits_{i=1}^m(\theta_0x^{(i)} - y^{(i)}x^{(i)}) &= 0\\
\theta_1\sum\limits_{i=1}^m((x^{(i)})^2 ) + \sum\limits_{i=1}^m(\theta_0x^{(i)} - y^{(i)}x^{(i)}) &= 0\\
\theta_1\sum\limits_{i=1}^m((x^{(i)})^2 )   &= - \sum\limits_{i=1}^m(\theta_0x^{(i)} - y^{(i)}x^{(i)})
\end{align}

Finally, divide both sides by the left sum to get an equation for $\theta_1$.

\begin{equation*}
\theta_1 = -\frac{\sum\limits_{i=1}^m(\theta_0x^{(i)} - y^{(i)}x^{(i)})}{\sum\limits_{i=1}^m((x^{(i)})^2 )}
\end{equation*}













\end{document}